To improve the performance of your Spark program, you can consider the following optimizations:

1. **Caching**: Cache the RDDs that are reused multiple times. In your case, you can cache the `cleanedInputRDD` as it is used for both counting and sorting. Caching can save recomputation and improve performance.

   ```java
   cleanedInputRDD.cache();
   ```

2. **Partitioning**: Spark automatically splits the data into partitions. You can adjust the number of partitions for better parallelism, depending on the available resources. You can use the `repartition()` method to control the number of partitions.

   ```java
   cleanedInputRDD = cleanedInputRDD.repartition(4); // Adjust the number of partitions as needed.
   ```

3. **Use `persist()`**: Instead of using `cache()`, you can also use `persist()` to control storage levels (e.g., MEMORY_ONLY, MEMORY_ONLY_SER, etc.). Choose the appropriate storage level based on your cluster configuration and memory availability.

   ```java
   cleanedInputRDD.persist(StorageLevel.MEMORY_ONLY);
   ```

4. **Avoid Shuffling**: Shuffling can be expensive. Try to minimize shuffling operations, especially for large datasets. In your code, the `reduceByKey` and `sortByKey` operations involve shuffling. If the data fits in memory, consider using `aggregateByKey` for the reduction step to minimize data shuffling.

5. **Broadcast Small Data**: If you have a small dataset that needs to be shared across all nodes, you can use the `broadcast()` function to efficiently broadcast the data to all worker nodes. This can help reduce data transfer overhead.

6. **Tune Spark Configuration**: Depending on your cluster resources, you may need to adjust Spark configuration settings such as `spark.executor.memory`, `spark.driver.memory`, `spark.executor.cores`, and others to optimize performance.

7. **Use Java Streams**: For small-scale processing like filtering and splitting, you can consider using Java Streams before converting the data into an RDD. Java Streams may be more efficient for these operations.

8. **Profile and Monitor**: Use Spark's built-in monitoring tools and profilers to identify performance bottlenecks. The Spark UI (http://localhost:4040) provides valuable information about your application's performance.

9. **Consider Using DataFrames/Datasets**: Depending on your specific use case, you may benefit from using Spark DataFrames or Datasets, which offer optimizations and a more user-friendly API for structured data.

10. **Cluster Resources**: Ensure that your cluster has enough resources (CPU, memory, etc.) to handle the workload efficiently. You may need to scale your cluster or allocate more resources if necessary.

By applying these optimizations, you can potentially improve the performance of your Spark program. Keep in mind that the effectiveness of these optimizations may vary depending on your specific dataset and cluster configuration, so it's essential to profile and monitor your application to identify the most significant bottlenecks.
